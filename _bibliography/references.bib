---
---

@Article{vaccines10101716,
bibtex_show={true},
website = {https://doi.org/10.3390/vaccines10101716},
abbr = {Vaccines},
PDF = {vaccines-10-01716.pdf},
AUTHOR = {Beerman, Jack T. and Beaumont, Gwendal and Giabbanelli, Philippe J.},
TITLE = {A Scoping Review of Three Dimensions for Long-Term COVID-19 Vaccination Models: Hybrid Immunity, Individual Drivers of Vaccinal Choice, and Human Errors},
JOURNAL = {Vaccines},
VOLUME = {10},
YEAR = {2022},
NUMBER = {10},
ARTICLE-NUMBER = {1716},
PubMedID = {36298581},
ISSN = {2076-393X},
ABSTRACT = {The virus that causes COVID-19 changes over time, occasionally leading to Variants of Interest (VOIs) and Variants of Concern (VOCs) that can behave differently with respect to detection kits, treatments, or vaccines. For instance, two vaccination doses were 61% effective against the BA.1 predominant variant, but only 24% effective when BA.2 became predominant. While doses still confer protection against severe disease outcomes, the BA.5 variant demonstrates the possibility that individuals who have received a few doses built for previous variants can still be infected with newer variants. As previous vaccines become less effective, new ones will be released to target specific variants and the whole process of vaccinating the population will restart. While previous models have detailed logistical aspects and disease progression, there are three additional key elements to model COVID-19 vaccination coverage in the long term. First, the willingness of the population to participate in regular vaccination campaigns is essential for long-term effective COVID-19 vaccination coverage. Previous research has shown that several categories of variables drive vaccination status: sociodemographic, health-related, psychological, and information-related constructs. However, the inclusion of these categories in future models raises questions about the identification of specific factors (e.g., which sociodemographic aspects?) and their operationalization (e.g., how to initialize agents with a plausible combination of factors?). While previous models separately accounted for natural- and vaccine-induced immunity, the reality is that a significant fraction of individuals will be both vaccinated and infected over the coming years. Modeling the decay in immunity with respect to new VOCs will thus need to account for hybrid immunity. Finally, models rarely assume that individuals make mistakes, even though this over-reliance on perfectly rational individuals can miss essential dynamics. Using the U.S. as a guiding example, our scoping review summarizes these aspects (vaccinal choice, immunity, and errors) through ten recommendations to support the modeling community in developing long-term COVID-19 vaccination models.},
DOI = {10.3390/vaccines10101716}
}

@INPROCEEDINGS{10155377,
bibtex_show={true},
website = {https://ieeexplore.ieee.org/document/10155377},
abbr = {ANNSIM},
PDF={annsim_2023.pdf},
author = {Beerman, Jack T. and Beaumont, Gwendal and Giabbanelli, Philippe J.},
booktitle = {2023 Annual Modeling and Simulation Conference (ANNSIM)},
title = {On the Necessity of Human Decision-Making Errors to Explain Vaccination Rates for Covid-19: an Agent-Based Modeling Study},
year = {2023},
pages = {413--424},
abstract = {COVID-19 vaccines are important for individuals to avoid severe illness and collectively to prevent significant societal disruptions from uncontrolled disease spread. Vaccine adoption depends both on objective data about vaccine efficiency and on perceptions, which are shaped by individual characteristics and peer influences. Despite the abundance of Agent-Based Models (ABMs) models for COVID-19 and the long-term need for booster doses, ABMs have not yet accounted for the interplay of individual and collective drivers of vaccine adoption. In this explanatory study, we modify the validated COVASIM framework such that agents observe their peers’ characteristics (derived from several datasets), use machine learning to reflect and then take decisions based on their own characteristics. We show that specific decision-making errors are necessary to replicate the real-world prevalence of COVID-19 vaccine coverage in the USA. Specifically, agents must only observe simple features of their peers (e.g., age, sex) rather than personal information (e.g., comorbidities).},
keywords = {covid-19;decision making;machine learning;agent-based modeling;vaccines;diseases},
doi = {},
url = {https://ieeexplore.ieee.org/document/10155377},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}

@article{BEERMAN2023102119,
bibtex_show={true},
website = {https://doi.org/10.1016/j.jocs.2023.102119},
abbr = {JCS},
PDF={jcs_2023.pdf},
title = {A framework for the comparison of errors in agent-based models using machine learning},
journal = {Journal of Computational Science},
volume = {72},
pages = {102--119},
year = {2023},
issn = {1877-7503},
doi = {10.1016/j.jocs.2023.102119},
author = {Beerman, Jack T. and Beaumont, Gwendal and Giabbanelli, Philippe J.},
keywords = {Agent-based modeling, Machine learning, Perceptual errors, Python},
abstract = {Across common ABM frameworks (e.g., BDI, SOAR, ACT-R) errors in human perceptions are inconsistently handled through implementation or lack thereof. Without the proper integration or absence of key human behaviors, researchers in cognitive social simulations may overlook various features that would have affected the overall model trajectory. We address this concern by developing a framework that illustrates the value of agents possessing internal models that are driven by Machine Learning. We illustrate the impact of this framework on three well-known models (Schelling, Sugarscape, Axelrod) and on a COVID-19 simulation. Our work employs various Machine Learning models (e.g., Decision Tree Classifier, Logistic Regressor) to depict how the inclusion of human errors alters the overall model trajectory and may justify the integration of imperfections and heterogeneity into individual decision-making processes. Our open source framework can be integrated into existing and future models and utilized to examine the consequences of an agent making a decision without the appropriate amount of information (insufficient observation), by ignoring specific information (superficial observation), when inaccurately recording information (inaccurate perception), or due to a gap between environmental complexity and individual capacity (limited ability).}
}

@inproceedings{beaumont2024towards,
bibtex_show={true},
website={https://doi.org/10.1145/3652620.3688259},
abbr={EDTconf},
PDF={edtconf_2024.pdf},
author = {Beaumont, Gwendal and Beugnard, Antoine and Mart\'{i}nez, Salvador and Urtado, Christelle and Vauttier, Sylvain},
title = {Towards Re-Engineering Digital Twins: Preliminary Experiments on Three Use Cases},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3652620.3688259},
abstract = {Digital Twins (DTs) are everywhere, but they often have been developed before consensual definitions, reference models or adapted tools were proposed. Thus, these ad hoc DTs cannot be easily manipulated as actual entities, hampering both their sustainability and their life cycle management (e.g., their redeployment). In order to deal with this issue, this paper chooses to learn from practice by re-engineering existing DTs. Starting from a generic a priori metamodel that tries to clearly separate concerns (so that DTs can be seen as first class software entities), this paper implements experiments in three steps: (i) reverse engineering an existing Digital Twin, seen as a prototypical instance, (ii) generalization of the DT architecture descriptor into a DT model, (iii) alignment between this reverse engineered DT model and the a priori DT metamodel. First experiments, run on three use cases, show that our proposal can relevantly analyze and document DTs to support their re-engineering as first class entities.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {453–-458},
numpages = {6},
keywords = {digital twin, metamodel, model-driven engineering, digital twin architecture, reverse engineering digital twins, re-engineering digital twins},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@InProceedings{beaumont2025towards,
bibtex_show={true},
website={https://doi.org/10.1007/978-3-032-08623-5_22},
abbr={ER},
PDF={er_2025.pdf},
selected={true},
author={Beaumont, Gwendal and Beugnard, Antoine and Mart\'{i}nez, Salvador and Urtado, Christelle and Vauttier, Sylvain},
editor={Bork, Dominik and Lukyanenko, Roman and Sadiq, Shazia and Bellatreche, Ladjel and Pastor, Oscar},
title={Towards Automating the Life Cycle Management of Digital Twins},
booktitle={Conceptual Modeling},
year={2025},
publisher={Springer Nature Switzerland},
address={Cham},
pages={412--430},
abstract={Digital Twins (DTs) are key to Industry 4.0 and IoT systems. By definition, as virtual counterparts of a Physical Object (PO), they have to be kept up-to-date with the PO at a fast pace, leading to multiple reconfigurations, reconnections and redeployments throughout their lifetime. On the one hand, there is a growing interest around DT platforms and DT modeling languages from both academia and industry. However, systematic operational methodologies and practical tools for managing DTs resources and life cycle are more scarce. This paper identifies the need for a domain-agnostic solution to systematically manage the DT life cycle. On the other hand, GitOps provides principles and concrete actionable tools, such as CI/CD pipelines, declarative cluster state and version control, to manage software architectures that needs to: (1) be redeployed swiftly without interruptions and, (2) be versioned. By combining the static Digital Twin Definition Language and the GitOps framework, a new way of automating life cycle management of DTs at runtime is proposed in this paper. A conceptualization of core DT life cycle operations at runtime such as connect, synchronize, or configure and a similar work on generic GitOps operations is provided. Then, a mapping from DT to generic GitOps operations is proposed. Finally, a proof-of-concept involving a Fischertechnik Training Factory 4.0 as a use-case is presented.},
isbn={978-3-032-08623-5},
doi={10.1007/978-3-032-08623-5_22}
}
